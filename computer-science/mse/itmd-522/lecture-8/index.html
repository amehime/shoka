<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="優萌初華" href="https://shoka.lostyu.me/rss.xml"><link rel="alternate" type="application/atom+xml" title="優萌初華" href="https://shoka.lostyu.me/atom.xml"><link rel="alternate" type="application/json" title="優萌初華" href="https://shoka.lostyu.me/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="机器学习,数据挖掘"><link rel="canonical" href="https://shoka.lostyu.me/computer-science/mse/itmd-522/lecture-8/"><meta name="description" content="# Feature Selection and Reduction 特征选择与约简   This is very important process in data analytics and data mining.   Reason why?  Not all of the features are useful  并非所有要素都有用 I rrelevant features will dec"><meta property="og:type" content="article"><meta property="og:title" content="Lecture 8. Feature Selection and Reduction &amp; Outlier Detection"><meta property="og:url" content="https://shoka.lostyu.me/computer-science/mse/itmd-522/lecture-8/"><meta property="og:site_name" content="優萌初華"><meta property="og:description" content="# Feature Selection and Reduction 特征选择与约简   This is very important process in data analytics and data mining.   Reason why?  Not all of the features are useful  并非所有要素都有用 I rrelevant features will dec"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s2.loli.net/2022/02/24/GHuOU8pgQLqv9Z2.png"><meta property="og:image" content="https://s2.loli.net/2022/02/24/rsSyoHf3W8Ie95Z.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/fH9DgAh4cBdlTML.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/MWCkOQ9PUdhB8pL.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/9yHASpsx2qdU6cj.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/JcH5eKxOlqiCUbA.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/jDqZbsvhOY1P5k8.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/TwN5IvGzc2De6o7.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/A1iYNQKSd7m4opL.png"><meta property="og:image" content="https://s2.loli.net/2022/02/25/CPsrnmuaAzx7lkp.png"><meta property="article:published_time" content="2022-02-23T15:02:22.000Z"><meta property="article:modified_time" content="2022-02-23T15:02:22.000Z"><meta property="article:author" content="Ruri Shimotsuki"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="数据挖掘"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://s2.loli.net/2022/02/24/GHuOU8pgQLqv9Z2.png"><meta name="twitter:creator" content="@amehime"><title>Lecture 8. Feature Selection and Reduction & Outlier Detection - ITMD 522. Data Mining and Machine Learning - Master of Software Engineering - 计算机科学 | Yume Shoka = 優萌初華 = 有夢書架</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Lecture 8. Feature Selection and Reduction & Outlier Detection</h1><div class="meta"><span class="item" title="创建时间：2022-02-23 23:02:22"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-02-23T23:02:22+08:00">2022-02-23</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>11k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>10 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Yume Shoka</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/86a28095f8e05aa07544b454ec6f01f6.jpg"></li><li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/9bb9e2a4cf749eb38bd38c97835e3366.jpg"></li><li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/4ae00b1d55fc55227c12fbe35843514e.jpg"></li><li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/e44fe20f43f92d83732bf40e7d675808.jpg"></li><li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/20179998b27b569ef4634293516e8d09.jpg"></li><li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/f584675dfc4ef47fcf19442f0e2ea18b.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/" itemprop="item" rel="index" title="分类于 计算机科学"><span itemprop="name">计算机科学</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/mse/" itemprop="item" rel="index" title="分类于 Master of Software Engineering"><span itemprop="name">Master of Software Engineering</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/mse/itmd-522/" itemprop="item" rel="index" title="分类于 ITMD 522. Data Mining and Machine Learning"><span itemprop="name">ITMD 522. Data Mining and Machine Learning</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://shoka.lostyu.me/computer-science/mse/itmd-522/lecture-8/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Ruri Shimotsuki"><meta itemprop="description" content="有夢書架, 琉璃的医学 & 编程笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="優萌初華"></span><div class="body md" itemprop="articleBody"><h1 id="feature-selection-and-reduction-特征选择与约简"><a class="anchor" href="#feature-selection-and-reduction-特征选择与约简">#</a> Feature Selection and Reduction 特征选择与约简</h1><ul><li><p>This is very important process in data analytics and data mining.</p></li><li><p>Reason why?</p><ul><li>Not all of the features are useful<br>并非所有要素都有用</li><li>I rrelevant features will decrease accuracy<br>相关要素会降低精确度</li><li>Data collection is an expensive process, you cannot simply remove features with your common sense<br>数据收集是一个昂贵的过程，不能简单地根据常识移除要素</li><li>You must remove features or reduce dimensions by specific reasons<br>必须基于特定原因移除要素或缩减维度</li></ul></li></ul><h2 id="major-techniques-of-dimensionality-reduction-降维的主要技巧"><a class="anchor" href="#major-techniques-of-dimensionality-reduction-降维的主要技巧">#</a> Major Techniques of Dimensionality Reduction 降维的主要技巧</h2><h3 id="feature-selection-特征选择"><a class="anchor" href="#feature-selection-特征选择">#</a> Feature Selection 特征选择</h3><dl><dt>Definition</dt><dd>A process that chooses an optimal subset of features according to a objective function<br>根据目标函数选择最佳特征子集的过程</dd><dt>Objectives</dt><dd><ul><li>To reduce dimensionality and remove noise<br>降低维数并去除噪声</li><li>To improve mining performance<br>提高挖掘性能<ul><li>Speed of learning<br>学习速度</li><li>Predictive accuracy<br>预测精度</li><li>Simplicity and comprehensibility of mined results<br>挖掘结果的简单性和可理解性</li></ul></li></ul></dd><dt>Output 输出</dt><dd>Only <ins>a subset of the original features</ins> are selected<br>只有一个子集的原始特性</dd></dl><blockquote><p>Feature Selection</p><ul><li>Filtering approach Kohavi and John, 1996</li><li>Wrapper approach Kohavi and John, 1996</li><li>Embedded methods I.Guyon et. al., 2006</li></ul></blockquote><h3 id="feature-extractionreduction-特征提取约简"><a class="anchor" href="#feature-extractionreduction-特征提取约简">#</a> Feature Extraction/Reduction 特征提取 / 约简</h3><dl><dt>Feature reduction</dt><dd><ul><li>refers to the mapping of the original high-dimensional data onto a lower-dimensional space<br>特征约简是指将原始的高维数据映射到低维空间</li><li>Given a set of data points of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span> variables <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{ x_{1}, x_{2}, \dots x_{n} \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">…</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span><br>Compute their low-dimensional representation:<br>计算它们的低维表示:<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="fraktur">R</mi><mi>d</mi></msup><mo>→</mo><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="fraktur">R</mi><mi>p</mi></msup><mo stretchy="false">(</mo><mi>p</mi><mo>&lt;</mo><mo>&lt;</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{i} \in \mathfrak{R}^{d} \rightarrow y_{i} \in \mathfrak{R}^{p}(p&lt;&lt;d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6891em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8991079999999999em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathfrak">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8991079999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.7335400000000001em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord mathfrak">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.714392em"><span style="top:-3.1130000000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span></span><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></span></p></li></ul></dd><dt>Criterion</dt><dd><ul><li>Criterion for feature reduction can be different based on different problem settings.<br>基于不同的问题设置，特征约简的标准可以不同。</li><li>Unsupervised setting: minimize the information loss, e.g., <code>PCA</code><br>无监督设置：最小化信息损失</li><li>Supervised setting: maximize the class discrimination, e.g., <code>LDA</code><br>监督设置：最大化阶级歧视</li></ul></dd><dt>Input</dt><dd>All original features are used<br>所有原始功能</dd><dt>Output</dt><dd>The <ins>transformed features</ins> are linear combinations of the original features<br>变换后的要素是原始要素的线性组合</dd></dl><blockquote><p>Dimensionality Reduction</p><ul><li>Principal Components Analysis (PCA)</li><li>Nonlinear PCA (Kernel PCA, CatPCA)</li><li>Multi Dimensional Scaling (MDS)</li><li>Homogeneity Analysis</li></ul></blockquote><h1 id="feature-selection-特征选择-2"><a class="anchor" href="#feature-selection-特征选择-2">#</a> Feature Selection 特征选择</h1><h2 id="components-in-feature-selection"><a class="anchor" href="#components-in-feature-selection">#</a> Components In Feature Selection</h2><ul><li>For every feature selection technique, there must be at least two components<br>对于每种特征选择技术，必须至少有两个组成部分<ul><li>Quality Measure<br>质量测量</li><li>Search/Rank Methods<br>搜索 / 排名方法</li></ul></li></ul><h3 id="example-linear-regression"><a class="anchor" href="#example-linear-regression">#</a> Example: Linear Regression</h3><ul><li><p>In linear regression, we are going to predict a numerical variable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span>, by using a set of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> variables, e.g., <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_{1}, x_{2}, x_{3}, \dots , x_{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">…</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span><br>在线性回归中，我们将通过使用一组<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 变量来预测数值变量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span></p></li><li><p>Search Methods</p><ul><li><p>Backward Elimination 反向消除<br>Use all <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> variables to build the model<br>使用所有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 变量来构建模型，<br>Drop <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> variables step by step to see whether we can improve the model<br>逐步删除<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 变量，看看我们是否可以改进模型</p></li><li><p>Forward Selection 正向选择<br>Build a simple model, e.g., a model with only one <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span><br>构建一个简单的模型，例如，只有一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 的模型<br>Try to add more <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> variables step by step to see whether we can improve the model<br>尝试逐步添加更多的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 变量，看看我们是否可以改进模型</p></li><li><p>Stepwise = Forward + Backward 向前 + 向后</p></li></ul></li><li><p>In linear regression, we discuss different ways to select independent variables to predict the dependent variable<br>在线性回归中，我们讨论了选择自变量来预测因变量的不同方法</p><blockquote><p><span class="blue">Search or Rank Method</span> 搜索 / 排名方法 ---- <span class="red">Quality Measures</span> 质量测量</p></blockquote><ul><li><span class="blue">Backward Elimination</span> by using <span class="red">p-value</span><br>利用<span class="red"> p 值</span><span class="blue">反向消除</span></li><li><span class="blue">Backward Elimination</span> by using <span class="red">AIC/BIC</span><br>利用<span class="red"> AIC/BIC</span><span class="blue"> 反向消除</span></li><li><span class="blue">Forward Selection</span> or Stepwise by using <span class="red">AIC/BIC</span><br>使用<span class="red"> AIC/BIC</span><span class="blue"> 正向选择</span>或逐步选择</li></ul></li></ul><h3 id="quality-measure-质量测量"><a class="anchor" href="#quality-measure-质量测量">#</a> Quality Measure 质量测量</h3><ul><li><p>The goodness of a feature/feature subset is dependent on measures<br>特征 / 特征子集的优点取决于测量</p></li><li><p>Various measures</p><ul><li>Information measures (Yu &amp; Liu 2004, Jebara &amp; Jaakkola 2000)</li><li>Distance measures (Robnik &amp; Kononenko 03, Pudil &amp; Novovicov 98)</li><li>Dependence measures (Hall 2000, Modrzejewski 1993)</li><li>Consistency measures (Almuallim &amp; Dietterich 94, Dash &amp; Liu 03)</li><li>Accuracy measures (Dash &amp; Liu 2000, Kohavi&amp;John 1997)</li></ul></li></ul><h4 id="information-measures"><a class="anchor" href="#information-measures">#</a> Information Measures</h4><ul><li>Entropy of variable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 变量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 的熵<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow><msub><mo><mi>log</mi><mo>⁡</mo></mo><mn>2</mn></msub><mrow><mo fence="true">(</mo><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(X)=-\sum_{i} P\left(x_{i}\right) \log _{2}\left(P\left(x_{i}\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.327674em;vertical-align:-1.277669em"></span><span class="mord">−</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0">)</span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop"><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0">)</span></span><span class="mclose delimcenter" style="top:0">)</span></span></span></span></span></span></p><blockquote><p>Impurity Measure 杂质测量</p></blockquote></li><li>Entropy of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> after observing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span></span></span></span> 观测<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span></span></span></span> 后<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 的熵<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo>∣</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>j</mi></munder><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><munder><mo>∑</mo><mi>i</mi></munder><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><msub><mo><mi>log</mi><mo>⁡</mo></mo><mn>2</mn></msub><mrow><mo fence="true">(</mo><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(X \mid Y)=-\sum_{j} P\left(y_{j}\right) \sum_{i} P\left(x_{i} \mid y_{j}\right) \log _{2}\left(P\left(x_{i} \mid y_{j}\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.463782em;vertical-align:-1.413777em"></span><span class="mord">−</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8723309999999997em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0">)</span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0">)</span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop"><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0">)</span></span><span class="mclose delimcenter" style="top:0">)</span></span></span></span></span></span></p></li><li>Information Gain 信息增益<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mi>G</mi><mo stretchy="false">(</mo><mi>X</mi><mo>∣</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo>∣</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I G(X \mid Y)=H(X)-H(X \mid Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span><span class="mclose">)</span></span></span></span></span></p></li></ul><blockquote><p>This measure is used in decision tree classification<br>该测量用于决策树分类</p></blockquote><h4 id="accuracy-measures-准确度测量"><a class="anchor" href="#accuracy-measures-准确度测量">#</a> Accuracy Measures 准确度测量</h4><ul><li><p>Using classification accuracy of a classifier as an evaluation measure<br>使用分类器的分类精度作为评估指标</p></li><li><p>Factors constraining the choice of measures<br>限制措施选择的因素</p><ul><li>Classifier being used<br>正在使用的分类器</li><li>The speed of building the classifier<br>构建分类器的速度</li></ul></li><li><p>Compared with previous measures 与之前的措施相比</p><ul><li>Directly aimed to improve accuracy<br>直接旨在提高准确性</li><li>Biased toward the classifier being used<br>偏向于正在使用的分类器</li><li>More time consuming<br>更耗时</li></ul></li></ul><h3 id="feature-search-特征搜索"><a class="anchor" href="#feature-search-特征搜索">#</a> Feature Search 特征搜索</h3><p><img data-src="https://s2.loli.net/2022/02/24/GHuOU8pgQLqv9Z2.png" alt="" title="Depth-first search 深度优先搜索"><br><img data-src="https://s2.loli.net/2022/02/24/rsSyoHf3W8Ie95Z.png" alt="" title="Breadth-first search 广度优先搜索"></p><h3 id="feature-ranking-特征排序"><a class="anchor" href="#feature-ranking-特征排序">#</a> Feature Ranking 特征排序</h3><ul><li>Weighting and ranking individual features<br>对单个特征进行加权和排序</li><li>Selecting top ranked ones for feature selection<br>选择排名靠前的特征</li><li>Advantages<ul><li>Efficient: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mclose">)</span></span></span></span> in terms of dimensionality <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span></li><li>Easy to implement<br>易于实现</li></ul></li><li>Disadvantages<ul><li>Hard to determine the threshold<br>很难确定阈值</li><li>Unable to consider correlation between features<br>不能考虑特征之间的相关性</li></ul></li></ul><h2 id="two-models-of-feature-selection"><a class="anchor" href="#two-models-of-feature-selection">#</a> Two Models of Feature Selection</h2><dl><dt>Filter model 过滤器模型</dt><dd><ul><li>Separating feature selection from classifier learning<br>从分类器学习中分离特征选择</li><li>Relying on general characteristics of data (information, distance, dependence, consistency)<br>依赖于数据的一般特征 (信息、距离、依赖性、一致性)</li><li>No bias toward any learning algorithm, fast running<br>对任何学习算法都没有偏见，快速运行<br><img data-src="https://s2.loli.net/2022/02/25/fH9DgAh4cBdlTML.png" alt="image.png" title="Filter Model"></li></ul></dd><dt>Wrapper model 包装器模型</dt><dd><ul><li>Relying on a pre-determined classification algorithm<br>依赖于预先确定的分类算法</li><li>Using predictive accuracy as goodness measure<br>使用预测精度作为优度测量</li><li>High accuracy, computationally expensive<br>高精度，计算成本高<br><img data-src="https://s2.loli.net/2022/02/25/MWCkOQ9PUdhB8pL.png" alt="" title="Wrapper Model"></li></ul></dd></dl><h1 id="feature-reduction-特征约简"><a class="anchor" href="#feature-reduction-特征约简">#</a> Feature Reduction 特征约简</h1><h2 id="feature-reduction-algorithms"><a class="anchor" href="#feature-reduction-algorithms">#</a> Feature Reduction Algorithms</h2><ul><li><p>Unsupervised</p><ul><li>Latent Semantic Indexing <code>LSI</code> : truncated SVD</li><li>Independent Component Analysis <code>ICA</code></li><li><span class="red">Principal Component Analysis</span> <code>PCA</code></li><li>Manifold learning algorithms</li></ul></li><li><p>Supervised</p><ul><li><span class="red">Linear Discriminant Analysis</span> <code>LDA</code></li><li>Canonical Correlation Analysis <code>CCA</code></li><li>Partial Least Squares <code>PLS</code></li></ul></li><li><p>Semi-supervised</p></li></ul><hr><dl><dt>Linear Discriminant Analysis <code>LDA</code> 线性判别分析</dt><dd>tries to identify attributes that account for the most variance between classes.<br>试图找出能够解释类之间差异最大的属性。<br>In particular, <code>LDA</code> , in contrast to <code>PCA</code> , is a supervised method, using known class labels.<br>特别是， <code>LDA</code> 相对于 <code>PCA</code> ，是一种有监督的方法，使用已知的类标签。</dd><dt>Principal Component Analysis <code>PCA</code> 主成分分析</dt><dd>applied to this data identifies the combination of linearly uncorrelated attributes (principal components, or directions in the feature space) that <span class="red">account for the most variance in the data</span>.<br>应用于这些数据，来识别线性不相关属性 (主成分，或特征空间中的方向) 的组合，这些属性解释了<span class="red">数据中最大的差异</span>。<br>Here we plot the different samples on the 2 first principal components.<br>这里我们把不同的样本绘制在两个第一主成分上。</dd><dt>Singular Value Decomposition <code>SVD</code> 奇异值分解</dt><dd>is a factorization of a real or complex matrix.<br>是实矩阵或复矩阵的因式分解。<br>Actually <code>SVD</code> was derived from <code>PCA</code> .<br>实际上， <code>SVD</code> 是从 <code>PCA</code> 中衍生出来的。</dd></dl><h2 id="principal-component-analysis-主成分分析"><a class="anchor" href="#principal-component-analysis-主成分分析">#</a> Principal Component Analysis 主成分分析</h2><h3 id="schemes"><a class="anchor" href="#schemes">#</a> Schemes</h3><p>Assume we have a data with multiple features<br>假设我们有一个具有多个特征的数据</p><ol><li>Try to find principle components(PCs) each component is a combination of the linearly uncorrelated attributes/features;<br>尝试寻找主成分 (PCs) 每个成分是线性不相关的属性 / 特征的组合</li><li>PCA allows to obtain an ordered list of those components that account for the largest amount of the variance from the data;<br>PCA 允许从数据中获得解释最大方差的那些组件的排序列表；</li><li>The amount of variance captured by the first component is larger than the amount of variance on the second component, and so on.<br>第一个组件捕获的差异量大于第二个组件的差异量，依此类推。</li><li>Then, we can reduce the dimensionality by ignoring the components with smaller contributions to the variance.<br>然后，我们可以通过忽略对方差贡献较小的分量来降低维数。</li><li>The final reduced features we have are no longer the original features, but the difference PCs, each PC is a linear combination of your original features.<br>我们最终减少的功能不再是原始功能，而是不同的 PCs，每台 PCs 都是原始功能的线性组合。</li></ol><h3 id="how-to-obtain-those-principal-components-如何获得这些主成分"><a class="anchor" href="#how-to-obtain-those-principal-components-如何获得这些主成分">#</a> How to obtain those principal components? 如何获得这些主成分？</h3><p>The basic principle or assumption in PCA is:<br>主成分分析的基本原理或假设是：</p><p>The eigenvector of a covariance matrix equal to a principal component, because <ins>the eigenvector with the largest eigenvalue is the direction along which the data set has the maximum variance</ins>.<br>协方差矩阵的特征向量等于主成分，因为<ins>具有最大特征值的特征向量是数据集具有最大方差的方向</ins>。</p><p>Each eigenvector is associated with a eigenvalue;<br>每个特征向量都与一个特征值相关联；</p><p>Eigenvalue ➡️ tells how much the variance is;<br>特征值➡️代表方差有多大；</p><p>Eigenvector ➡️ tells the direction of the variation;<br>特征向量➡️代表变化的方向；</p><p>The next step: how to get the covariance matrix and how to calculate the eigenvectors/eigenvalues?<br>下一步：如何获得协方差矩阵以及如何计算特征向量 / 特征值？</p><h3 id="visualization-of-pca-可视化"><a class="anchor" href="#visualization-of-pca-可视化">#</a> Visualization of PCA 可视化</h3><p><img data-src="https://s2.loli.net/2022/02/25/9yHASpsx2qdU6cj.png" alt="" title="Example: Gene Expression"></p><p>The original expression by 3 genres is projected to two new dimensions, Such two dimensional visualization of the samples allow us to draw qualitative conclusions about the separability of experimental conditions (marked by different colors).<br>原来的三种类型的表达式被投影到两个新的维度，这种样本的二维可视化允许我们对实验条件 (用不同的颜色标记) 的可分性得出定性的结论。</p><h1 id="anomalyoutlier-detection-异常离群值检测"><a class="anchor" href="#anomalyoutlier-detection-异常离群值检测">#</a> Anomaly/Outlier Detection 异常 / 离群值检测</h1><ul><li><p>What are anomalies/outliers? 什么是异常 / 离群值</p><ul><li>The set of data points that are considerably different than the remainder or the majority of the data<br>与剩余数据或大部分数据有很大差异的数据点集</li></ul></li><li><p>Variants of Anomaly/Outlier Detection Problems 异常 / 离群值检测问题的变体</p><ul><li>Given a database <code>D</code> , find all the data points <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">x \in D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span> with anomaly scores greater than some threshold <code>t</code><br>给定一个数据库 <code>D</code> ，在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">x \in D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span> 中查找异常分数大于某个阈值 <code>t</code> 的所有数据点</li><li>Given a database <code>D</code> , find all the data points <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">x \in D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span> having the top-n largest anomaly scores <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><br>给定一个数据库 <code>D</code> ，找出所有数据点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">x \in D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span> 的前 n 个最大异常得分<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></li><li>Given a database <code>D</code> , containing mostly normal (but unlabeled) data points, and a test point <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> , compute the anomaly score of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> with respect to <code>D</code><br>给定一个数据库 <code>D</code> ，其中包含大部分正常（但未标记）数据点和一个测试点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span>，计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 关于 <code>D</code> 的异常分数</li></ul></li><li><p>Applications 应用</p><ul><li>Credit card fraud detection 信用卡欺诈检测</li><li>telecommunication fraud detection 电信欺诈检测</li><li>network intrusion detection 网络入侵检测</li><li>fault detection 故障检测</li></ul></li></ul><h2 id="anomaly-detection-schemes"><a class="anchor" href="#anomaly-detection-schemes">#</a> Anomaly Detection Schemes</h2><ul><li><p>General Steps:<br>一般步骤</p><ol><li><p>Build a profile of the “normal” behavior<br>建立 “正常” 行为的轮廓</p><ul><li>Profile can be patterns or summary statistics for the overall population<br>轮廓可以是总体群体的模式或汇总统计数据</li></ul></li><li><p>Use the “normal” profile to detect anomalies<br>使用 “正常” 轮廓检测异常</p><ul><li>Anomalies are observations whose characteristics differ significantly from the normal profile<br>异常是特征与正常轮廓显著不同的观察结果</li></ul></li></ol></li><li><p>Types of anomaly detection schemes<br>异常检测方案的类型</p><ul><li>Graphical 图形</li><li>Model based 基于模型</li><li>Distance based 基于距离</li><li>Clustering based 基于聚类</li></ul></li></ul><p><img data-src="https://s2.loli.net/2022/02/25/JcH5eKxOlqiCUbA.png" alt=""></p><h3 id="graphical-approaches-图解法"><a class="anchor" href="#graphical-approaches-图解法">#</a> Graphical Approaches 图解法</h3><p><code>Boxplot (1-D), Scatter plot (2-D), Spin plot (3-D)</code></p><p class="gallery"><img data-src="https://s2.loli.net/2022/02/25/jDqZbsvhOY1P5k8.png" alt=""><br><img data-src="https://s2.loli.net/2022/02/25/TwN5IvGzc2De6o7.png" alt=""></p><ul><li>Limitations 限制<ul><li>Time consuming 耗时</li><li>Subjective 主观</li></ul></li></ul><h3 id="statistical-approaches-model-based-统计方法-基于模型"><a class="anchor" href="#statistical-approaches-model-based-统计方法-基于模型">#</a> Statistical Approaches - Model based 统计方法 - 基于模型</h3><ul><li>Assume a parametric model describing the distribution of the data (e.g., normal distribution)<br>假设参数模型描述数据的分布 (例如，正态分布)</li><li>Apply a statistical test that depends on<br>应用统计检验，该检验取决于<ul><li>Data distribution<br>数据分布</li><li>Parameter of distribution (e.g., mean, variance)<br>分布的参数 (例如，均值、方差)</li><li>Number of expected outliers (confidence limit)<br>预期离群值的数量 (置信极限)</li></ul></li></ul><p><img data-src="https://s2.loli.net/2022/02/25/A1iYNQKSd7m4opL.png" alt=""></p><h3 id="distance-based-approaches-基于距离的方法"><a class="anchor" href="#distance-based-approaches-基于距离的方法">#</a> Distance-based Approaches 基于距离的方法</h3><ul><li>Data is represented as a vector of features<br>数据表示为要素矢量</li><li>Three major approaches 三种主要方法<ul><li>Nearest neighbor based 基于最近邻</li><li>Density based 基于密度</li><li>Clustering based 基于聚类</li></ul></li></ul><h4 id="nearest-neighbor-based-approach"><a class="anchor" href="#nearest-neighbor-based-approach">#</a> Nearest-Neighbor Based Approach</h4><ul><li>Compute the distance between every pair of data points<br>计算每对数据点之间的距离</li><li>There are various ways to define outliers:<br>定义异常值的方法有多种：<ul><li>Data points for which there are fewer than p neighboring points within a distance D<br>距离 D 内邻接点少于 p 个的数据点</li><li>The top n data points whose distance to the kth nearest neighbor is greatest<br>与第 k 个最近邻点的距离最大的前 n 个数据点</li><li>The top n data points whose average distance to the k nearest neighbors is greatest<br>到 k 个最近邻域的平均距离最大的前 n 个数据点</li></ul></li></ul><h4 id="clustering-based"><a class="anchor" href="#clustering-based">#</a> Clustering-Based</h4><ul><li>Idea: Use a clustering algorithm that has some notion of outliers!<br>想法：使用具有离群值概念的聚类算法！</li><li>The data which are far away from the centroid could be outliers<br>远离质心的数据可能是离群值</li><li>The set of data in a small cluster could be outliers<br>一个小聚类中的数据集可能是离群值</li></ul><h4 id="density-based-lof-approach"><a class="anchor" href="#density-based-lof-approach">#</a> Density-based: LOF approach</h4><ul><li>For each point, compute the density of its local neighborhood; e.g. use DBSCAN’s approach<br>对于每个点，计算其局部邻域的密度；例如，使用 DBSCAN 的方法</li><li>Compute local outlier factor (LOF) of a sample p as the average of the ratios of the density of sample p and the density of its nearest neighbors<br>计算样本 p 的局部异常值因子 (LOF) 为样本 p 的密度与其最近邻域的密度之比的平均值</li><li>Outliers are points with largest LOF value<br>离群值是 LOF 值最大的点</li></ul><p><img data-src="https://s2.loli.net/2022/02/25/CPsrnmuaAzx7lkp.png" alt=""></p><blockquote><p>In the NN approach, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">o_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> is not considered as outlier, while LOF approach find both <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">o_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">o_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> as outliers<br>在神经网络方法中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">o_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 不被认为是异常值，而 LOF 方法发现<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">o_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">o_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 都是异常值</p></blockquote><ul><li>Alternative approach: directly use density function; e.g. DENCLUE’s density function<br>另一种方法：直接使用密度函数，例如登克莱密度函数</li></ul><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 机器学习</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag"><i class="ic i-tag"></i> 数据挖掘</a></div></div><footer><div class="meta"><span id="computer-science/mse/itmd-522/lecture-8/" class="item leancloud_visitors" data-flag-title="Lecture 8. Feature Selection and Reduction & Outlier Detection" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Ruri Shimotsuki 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Ruri Shimotsuki 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="Ruri Shimotsuki 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Ruri Shimotsuki <i class="ic i-at"><em>@</em></i>優萌初華</li><li class="link"><strong>本文链接：</strong> <a href="https://shoka.lostyu.me/computer-science/mse/itmd-522/lecture-8/" title="Lecture 8. Feature Selection and Reduction &amp; Outlier Detection">https://shoka.lostyu.me/computer-science/mse/itmd-522/lecture-8/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/computer-science/mse/itmd-522/lecture-7/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;img.timelessq.com&#x2F;images&#x2F;2022&#x2F;07&#x2F;26&#x2F;ce11b4684a2ba0e3c7c3749e33fdd4ec.jpg" title="Lecture 7. Association Rules &amp; Web Mining"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> ITMD 522. Data Mining and Machine Learning</span><h3>Lecture 7. Association Rules & Web Mining</h3></a></div><div class="item right"><a href="/computer-science/mse/itmd-511/chapter-5-6/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;img.timelessq.com&#x2F;images&#x2F;2022&#x2F;07&#x2F;26&#x2F;ebaf5d5dbf78e8c31831feb203aa81fb.jpg" title="Chapter 5 &amp; 6"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> ITMD 511. Application Development Methodologies</span><h3>Chapter 5 & 6</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-selection-and-reduction-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%BA%A6%E7%AE%80"><span class="toc-number">1.</span> <span class="toc-text">Feature Selection and Reduction 特征选择与约简</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#major-techniques-of-dimensionality-reduction-%E9%99%8D%E7%BB%B4%E7%9A%84%E4%B8%BB%E8%A6%81%E6%8A%80%E5%B7%A7"><span class="toc-number">1.1.</span> <span class="toc-text">Major Techniques of Dimensionality Reduction 降维的主要技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-selection-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.</span> <span class="toc-text">Feature Selection 特征选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-extractionreduction-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BA%A6%E7%AE%80"><span class="toc-number">1.1.2.</span> <span class="toc-text">Feature Extraction&#x2F;Reduction 特征提取 &#x2F; 约简</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-selection-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-2"><span class="toc-number">2.</span> <span class="toc-text">Feature Selection 特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#components-in-feature-selection"><span class="toc-number">2.1.</span> <span class="toc-text">Components In Feature Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#example-linear-regression"><span class="toc-number">2.1.1.</span> <span class="toc-text">Example: Linear Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#quality-measure-%E8%B4%A8%E9%87%8F%E6%B5%8B%E9%87%8F"><span class="toc-number">2.1.2.</span> <span class="toc-text">Quality Measure 质量测量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#information-measures"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Information Measures</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#accuracy-measures-%E5%87%86%E7%A1%AE%E5%BA%A6%E6%B5%8B%E9%87%8F"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">Accuracy Measures 准确度测量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-search-%E7%89%B9%E5%BE%81%E6%90%9C%E7%B4%A2"><span class="toc-number">2.1.3.</span> <span class="toc-text">Feature Search 特征搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-ranking-%E7%89%B9%E5%BE%81%E6%8E%92%E5%BA%8F"><span class="toc-number">2.1.4.</span> <span class="toc-text">Feature Ranking 特征排序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#two-models-of-feature-selection"><span class="toc-number">2.2.</span> <span class="toc-text">Two Models of Feature Selection</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-reduction-%E7%89%B9%E5%BE%81%E7%BA%A6%E7%AE%80"><span class="toc-number">3.</span> <span class="toc-text">Feature Reduction 特征约简</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-reduction-algorithms"><span class="toc-number">3.1.</span> <span class="toc-text">Feature Reduction Algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#principal-component-analysis-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">3.2.</span> <span class="toc-text">Principal Component Analysis 主成分分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#schemes"><span class="toc-number">3.2.1.</span> <span class="toc-text">Schemes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-obtain-those-principal-components-%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97%E8%BF%99%E4%BA%9B%E4%B8%BB%E6%88%90%E5%88%86"><span class="toc-number">3.2.2.</span> <span class="toc-text">How to obtain those principal components? 如何获得这些主成分？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization-of-pca-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">3.2.3.</span> <span class="toc-text">Visualization of PCA 可视化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#anomalyoutlier-detection-%E5%BC%82%E5%B8%B8%E7%A6%BB%E7%BE%A4%E5%80%BC%E6%A3%80%E6%B5%8B"><span class="toc-number">4.</span> <span class="toc-text">Anomaly&#x2F;Outlier Detection 异常 &#x2F; 离群值检测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#anomaly-detection-schemes"><span class="toc-number">4.1.</span> <span class="toc-text">Anomaly Detection Schemes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#graphical-approaches-%E5%9B%BE%E8%A7%A3%E6%B3%95"><span class="toc-number">4.1.1.</span> <span class="toc-text">Graphical Approaches 图解法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#statistical-approaches-model-based-%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">Statistical Approaches - Model based 统计方法 - 基于模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distance-based-approaches-%E5%9F%BA%E4%BA%8E%E8%B7%9D%E7%A6%BB%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.3.</span> <span class="toc-text">Distance-based Approaches 基于距离的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nearest-neighbor-based-approach"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">Nearest-Neighbor Based Approach</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#clustering-based"><span class="toc-number">4.1.3.2.</span> <span class="toc-text">Clustering-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#density-based-lof-approach"><span class="toc-number">4.1.3.3.</span> <span class="toc-text">Density-based: LOF approach</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/computer-science/mse/itmd-522/lecture-1/" rel="bookmark" title="Lecture 1. KDD Process: Data PreProcessing 数据预处理">Lecture 1. KDD Process: Data PreProcessing 数据预处理</a></li><li><a href="/computer-science/mse/itmd-522/lecture-2/" rel="bookmark" title="Lecture 2. Supervised Learning 监督学习">Lecture 2. Supervised Learning 监督学习</a></li><li><a href="/computer-science/mse/itmd-522/lecture-3/" rel="bookmark" title="Lecture 3. Supervised Learning: Classification Algorithms Part.1 监督学习:分类算法1">Lecture 3. Supervised Learning: Classification Algorithms Part.1 监督学习:分类算法1</a></li><li><a href="/computer-science/mse/itmd-522/lecture-4/" rel="bookmark" title="Lecture 4. Supervised Learning: Classification Algorithms Part.2 监督学习:分类算法2">Lecture 4. Supervised Learning: Classification Algorithms Part.2 监督学习:分类算法2</a></li><li><a href="/computer-science/mse/itmd-522/lecture-5/" rel="bookmark" title="Lecture 5. Supervised Learning: Classification Algorithms Part.3 监督学习:分类算法3">Lecture 5. Supervised Learning: Classification Algorithms Part.3 监督学习:分类算法3</a></li><li><a href="/computer-science/mse/itmd-522/lecture-6/" rel="bookmark" title="Lecture 6. Unsupervised Learning: Clustering Techniques 无监督学习:聚类技术">Lecture 6. Unsupervised Learning: Clustering Techniques 无监督学习:聚类技术</a></li><li><a href="/computer-science/mse/itmd-522/lecture-7/" rel="bookmark" title="Lecture 7. Association Rules & Web Mining">Lecture 7. Association Rules & Web Mining</a></li><li class="active"><a href="/computer-science/mse/itmd-522/lecture-8/" rel="bookmark" title="Lecture 8. Feature Selection and Reduction & Outlier Detection">Lecture 8. Feature Selection and Reduction & Outlier Detection</a></li><li><a href="/computer-science/mse/itmd-522/mid-term/" rel="bookmark" title="Midterm Review">Midterm Review</a></li><li><a href="/computer-science/mse/itmd-522/lecture-9/" rel="bookmark" title="Lecture 9. Data Manipulation with Pandas">Lecture 9. Data Manipulation with Pandas</a></li><li><a href="/computer-science/mse/itmd-522/lecture-10/" rel="bookmark" title="Lecture 10. Classifications by Python">Lecture 10. Classifications by Python</a></li><li><a href="/computer-science/mse/itmd-522/lecture-11/" rel="bookmark" title="Lecture 11. Python for Unsupervised Learning">Lecture 11. Python for Unsupervised Learning</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Ruri Shimotsuki" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Ruri Shimotsuki</p><div class="description" itemprop="description">琉璃的医学 & 编程笔记</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">303</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">68</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">73</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWU=" title="https:&#x2F;&#x2F;github.com&#x2F;amehime"><i class="ic i-github"></i></span> <span class="exturl item twitter" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9hbWVoaW1l" title="https:&#x2F;&#x2F;twitter.com&#x2F;amehime"><i class="ic i-twitter"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9hbWVoaW1l" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;amehime"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTEyODg2ODIz" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;12886823"><i class="ic i-cloud-music"></i></span> <span class="exturl item telegram" data-url="aHR0cHM6Ly90Lm1lL2FtZWhpbWU=" title="https:&#x2F;&#x2F;t.me&#x2F;amehime"><i class="ic i-paper-plane"></i></span> <span class="exturl item about" data-url="aHR0cHM6Ly9hYm91dC5tZS9hbWVoaW1l" title="https:&#x2F;&#x2F;about.me&#x2F;amehime"><i class="ic i-address-card"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a><ul class="submenu"><li class="item"><a href="/about/yume" rel="section"><i class="ic i-cloud"></i>自设</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-magic"></i>链环</a><ul class="submenu"><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/webstack/" rel="section"><i class="ic i-star"></i>网址</a></li></ul></li><li class="item"><span class="exturl" data-url="aHR0cHM6Ly90cmF2ZWxsaW5ncy5saW5r"><i class="ic i-paper-plane"></i>开往</span></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/computer-science/mse/itmd-522/lecture-7/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/computer-science/mse/itmd-511/chapter-5-6/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/" title="分类于 Master of Software Engineering">Master of Software Engineering</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/itmd-514/" title="分类于 ITMD 514. Programming for Data Analytics">ITMD 514. Programming for Data Analytics</a></div><span><a href="/computer-science/mse/itmd-514/week-5/" title="Week 5. Exploratory Data Analysis (EDA) 探索性数据分析">Week 5. Exploratory Data Analysis (EDA) 探索性数据分析</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/" title="分类于 Master of Software Engineering">Master of Software Engineering</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/itmd-515/" title="分类于 ITMD 515. Advanced Software Programming">ITMD 515. Advanced Software Programming</a></div><span><a href="/computer-science/mse/itmd-515/lab-3/" title="Lab 3. Junit, JDBC and Bean Validation">Lab 3. Junit, JDBC and Bean Validation</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/" title="分类于 Master of Software Engineering">Master of Software Engineering</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/itmd-536/" title="分类于 ITMD 536. Software Testing and Maintenance">ITMD 536. Software Testing and Maintenance</a></div><span><a href="/computer-science/mse/itmd-536/lecture-5/" title="Lecture 5. Test Management &amp; Tool Support for Testing">Lecture 5. Test Management & Tool Support for Testing</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/" title="分类于 Master of Software Engineering">Master of Software Engineering</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/itmd-511/" title="分类于 ITMD 511. Application Development Methodologies">ITMD 511. Application Development Methodologies</a></div><span><a href="/computer-science/mse/itmd-511/chapter-12-13/" title="Chapter 12 &amp; 13">Chapter 12 & 13</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/certificate/" title="分类于 证书">证书</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/certificate/ispm/" title="分类于 软考高项">软考高项</a></div><span><a href="/computer-science/certificate/ispm/chapter-5/" title="第5章 项目范围管理">第5章 项目范围管理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/clinical-medicine/" title="分类于 临床医学">临床医学</a> <i class="ic i-angle-right"></i> <a href="/categories/clinical-medicine/graduate/" title="分类于 研究生入学考试">研究生入学考试</a> <i class="ic i-angle-right"></i> <a href="/categories/clinical-medicine/graduate/course-1/" title="分类于 西医综合 - 内科学">西医综合 - 内科学</a></div><span><a href="/clinical-medicine/graduate/course-1/chapter-3/" title="第03章 肺结核">第03章 肺结核</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/" title="分类于 Master of Software Engineering">Master of Software Engineering</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/itmd-522/" title="分类于 ITMD 522. Data Mining and Machine Learning">ITMD 522. Data Mining and Machine Learning</a></div><span><a href="/computer-science/mse/itmd-522/lecture-10/" title="Lecture 10. Classifications by Python">Lecture 10. Classifications by Python</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/note/" title="分类于 二进制杂谈">二进制杂谈</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/note/theme-shoka-doc/" title="分类于 Theme Shoka Documentation">Theme Shoka Documentation</a></div><span><a href="/computer-science/note/theme-shoka-doc/" title="Hexo主题Shoka &amp; multi-markdown-it渲染器使用说明">Hexo主题Shoka & multi-markdown-it渲染器使用说明</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/" title="分类于 Master of Software Engineering">Master of Software Engineering</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/mse/itmd-510/" title="分类于 ITMD 510. Object-Oriented App Develop">ITMD 510. Object-Oriented App Develop</a></div><span><a href="/computer-science/mse/itmd-510/week-13/" title="Week 13. Object Serialization, Network Programming">Week 13. Object Serialization, Network Programming</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/clinical-medicine/" title="分类于 临床医学">临床医学</a> <i class="ic i-angle-right"></i> <a href="/categories/clinical-medicine/cardiovascular/" title="分类于 心血管病学">心血管病学</a> <i class="ic i-angle-right"></i> <a href="/categories/clinical-medicine/cardiovascular/echocardiography/" title="分类于 超声心动图">超声心动图</a> <i class="ic i-angle-right"></i> <a href="/categories/clinical-medicine/cardiovascular/echocardiography/valves/" title="分类于 第二阶段 Valves">第二阶段 Valves</a></div><span><a href="/clinical-medicine/cardiovascular/echocardiography/valves/course-1/" title="01. Echocardiography in Systemic Disease 超声心动图在系统性疾病中的运用">01. Echocardiography in Systemic Disease 超声心动图在系统性疾病中的运用</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Ruri Shimotsuki @ Yume Shoka</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">3.4m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">51:02</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"computer-science/mse/itmd-522/lecture-8/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?60ab4a51924a588bb4d810652c9412f1";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></body></html>